{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UfPLCK5_h7tC_RptBviX4IJATTLsQy0M","timestamp":1686056289909},{"file_id":"1ws14etu7cc0FHcr2y47jA7Pi3jzW-m_X","timestamp":1685995042647}],"authorship_tag":"ABX9TyP9DNffhVez58HpbqoB3jlt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"awJSrtgbkHqi"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"HtJdOKb6Ieja","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686056651862,"user_tz":-420,"elapsed":57295,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"b531ad63-7540-42fb-da75-6b6b50cda611"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Epoch 1/25\n","2/2 [==============================] - 1s 48ms/step - loss: 0.8247 - binary_accuracy: 0.5506\n","Epoch 2/25\n","2/2 [==============================] - 0s 49ms/step - loss: 0.4509 - binary_accuracy: 0.8442\n","Epoch 3/25\n","2/2 [==============================] - 0s 43ms/step - loss: 0.2962 - binary_accuracy: 0.9050\n","Epoch 4/25\n","2/2 [==============================] - 0s 41ms/step - loss: 0.1982 - binary_accuracy: 0.9544\n","Epoch 5/25\n","2/2 [==============================] - 0s 41ms/step - loss: 0.1428 - binary_accuracy: 0.9716\n","Epoch 6/25\n","2/2 [==============================] - 0s 45ms/step - loss: 0.1105 - binary_accuracy: 0.9766\n","Epoch 7/25\n","2/2 [==============================] - 0s 45ms/step - loss: 0.0853 - binary_accuracy: 0.9811\n","Epoch 8/25\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0720 - binary_accuracy: 0.9811\n","Epoch 9/25\n","2/2 [==============================] - 0s 50ms/step - loss: 0.0596 - binary_accuracy: 0.9848\n","Epoch 10/25\n","2/2 [==============================] - 0s 44ms/step - loss: 0.0467 - binary_accuracy: 0.9881\n","Epoch 11/25\n","2/2 [==============================] - 0s 48ms/step - loss: 0.0395 - binary_accuracy: 0.9877\n","Epoch 12/25\n","2/2 [==============================] - 0s 42ms/step - loss: 0.0335 - binary_accuracy: 0.9889\n","Epoch 13/25\n","2/2 [==============================] - 0s 48ms/step - loss: 0.0293 - binary_accuracy: 0.9893\n","Epoch 14/25\n","2/2 [==============================] - 0s 41ms/step - loss: 0.0237 - binary_accuracy: 0.9922\n","Epoch 15/25\n","2/2 [==============================] - 0s 48ms/step - loss: 0.0199 - binary_accuracy: 0.9951\n","Epoch 16/25\n","2/2 [==============================] - 0s 44ms/step - loss: 0.0170 - binary_accuracy: 0.9951\n","Epoch 17/25\n","2/2 [==============================] - 0s 41ms/step - loss: 0.0142 - binary_accuracy: 0.9955\n","Epoch 18/25\n","2/2 [==============================] - 0s 58ms/step - loss: 0.0117 - binary_accuracy: 0.9959\n","Epoch 19/25\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0095 - binary_accuracy: 0.9975\n","Epoch 20/25\n","2/2 [==============================] - 0s 49ms/step - loss: 0.0086 - binary_accuracy: 0.9988\n","Epoch 21/25\n","2/2 [==============================] - 0s 44ms/step - loss: 0.0074 - binary_accuracy: 0.9996\n","Epoch 22/25\n","2/2 [==============================] - 0s 39ms/step - loss: 0.0063 - binary_accuracy: 0.9996\n","Epoch 23/25\n","2/2 [==============================] - 0s 39ms/step - loss: 0.0054 - binary_accuracy: 0.9992\n","Epoch 24/25\n","2/2 [==============================] - 0s 67ms/step - loss: 0.0047 - binary_accuracy: 0.9992\n","Epoch 25/25\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0041 - binary_accuracy: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]}],"source":["import logging\n","import re\n","import string\n","import time\n","import ast\n","import pandas as pd\n","from google.colab import drive\n","from typing import Tuple, Union, List, Dict\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","\n","level = logging.INFO\n","logging.basicConfig(level=level)\n","logger = logging.getLogger(__name__)\n","\n","\n","class TFModel(tf.Module):\n","    def __init__(self, model: tf.keras.Model) -> None:\n","        self.model = model\n","\n","class ModelTrainer:\n","    def __init__(self) -> None:\n","        self.tf_model_wrapper: TFModel\n","\n","        # Model Architecture parameters\n","        self.max_features = 50000\n","        self.epochs = 25\n","        self.batch_size = 64\n","        self.padding_token = \"<pad>\"\n","        self.auto = tf.data.AUTOTUNE\n","\n","    def read_train(self, dir_train):\n","      train_df = pd.read_csv(dir_train, index_col=0)\n","      train_df['fixedFee_tags'] = train_df['fixedFee_tags'].apply(ast.literal_eval)\n","      train_df['text'] = train_df['text'].apply(str)\n","      return train_df\n","\n","    def vocabulary_size(self, train_df):\n","      vocabulary = set()\n","      train_df[\"text\"].str.lower().str.split().apply(vocabulary.update)\n","      vocabulary_size = len(vocabulary)\n","      return vocabulary_size\n","\n","    def make_dataset(self, train_df, is_train=True):\n","      labels = tf.ragged.constant(train_df[\"fixedFee_tags\"].values)\n","      lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n","      lookup.adapt(labels)\n","      label_binarized = lookup(labels).numpy()\n","      dataset = tf.data.Dataset.from_tensor_slices(\n","          (train_df[\"text\"].values, label_binarized)\n","        )\n","      dataset = dataset.shuffle(self.batch_size) if is_train else dataset\n","      return dataset.batch(self.batch_size)\n","\n","    def dataset(self, train_df):\n","      train_dataset = self.make_dataset(train_df, is_train = True)\n","      text_batch, label_batch = next(iter(train_dataset))\n","      text_batch = text_batch.numpy()\n","      label_batch = label_batch.numpy()\n","      return text_batch, label_batch\n","\n","    def init_vectorize_layer(self, vocabulary_size, text_dataset: np.ndarray) -> TextVectorization:\n","      text_vectorizer = TextVectorization(max_tokens=vocabulary_size,\n","                                          ngrams=2,\n","                                          output_mode='tf_idf')\n","      with tf.device(\"/CPU:0\"):\n","        text_vectorizer.adapt(text_dataset)\n","      return text_vectorizer\n","\n","    def init_model(self, train_df, vocabulary_size, text_dataset: np.ndarray) -> tf.keras.Model:\n","        text_batch, label_batch = self.dataset(train_df)\n","        vectorize_layer = self.init_vectorize_layer(text_dataset=text_batch, \n","                                                    vocabulary_size=vocabulary_size)\n","        raw_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n","        x = vectorize_layer(raw_input)\n","        x = tf.keras.layers.Dense(512, activation='relu')(x)\n","        x = tf.keras.layers.Dense(256, activation='relu')(x)\n","        x = tf.keras.layers.Dense(128, activation='relu')(x)\n","        predictions = tf.keras.layers.Dense(38, \n","                                            activation='sigmoid')(x)\n","        model = tf.keras.Model(raw_input, predictions)\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","        return model\n","\n","    def train(self) -> None:\n","        drive.mount('/content/drive')\n","        dir = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/service_budget_train_df.csv'  \n","        train_df = self.read_train(dir)\n","        vocabulary_size = self.vocabulary_size(train_df)\n","        text_batch, label_batch = self.dataset(train_df)\n","        model = self.init_model(train_df, text_dataset=text_batch, \n","                                vocabulary_size=vocabulary_size)\n","        model.fit(text_batch, label_batch, epochs=self.epochs)\n","        self.tf_model_wrapper = TFModel(model)\n","        path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/services/'\n","        model.save(path + 'budget_label_model/my_model')\n","        logger.info('saving SavedModel to budget_label_model/my_models')\n","\n","if __name__ == '__main__':\n","    model_trainer = ModelTrainer()\n","    model_trainer.train()"]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"JBtnMg0pkKYL"}},{"cell_type":"markdown","source":["### Load Model"],"metadata":{"id":"GDXPV7znklci"}},{"cell_type":"code","source":["path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/services/'\n","service_budget_model = tf.keras.models.load_model(path + 'budget_label_model/my_model')"],"metadata":{"id":"LbHtXUnckkxR","executionInfo":{"status":"ok","timestamp":1686056652822,"user_tz":-420,"elapsed":979,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### read vocab and make predict"],"metadata":{"id":"bp1xqvPKktL3"}},{"cell_type":"code","source":["vocab = []\n","with open(r'/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/service_budget_vocab.txt', 'r') as fp:\n","  for line in fp:\n","    x = line[:-1]\n","    vocab.append(x)\n","\n","user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = service_budget_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = [x for _, x in sorted(zip(predicted_probabilities[i], vocab),\n","                                       key=lambda pair: pair[0],\n","                                       reverse=True)][:2]\n","    print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUO-nnwMkLnc","executionInfo":{"status":"ok","timestamp":1686056669883,"user_tz":-420,"elapsed":17067,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"ca567ce2-56c6-4c27-b231-a4ba10df42f5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): jasa service hp\n","['200 - 250 Ribu', '20 - 50 Ribu']\n"]}]},{"cell_type":"code","source":["user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = service_budget_model(user_input)\n","for i, text in enumerate(user_input):\n","    print(f\"Text: {text}\")\n","    prediction = sorted(zip(predicted_probabilities[i], vocab),\n","                        key=lambda pair: pair[0],\n","                        reverse=True)\n","    top_1 = prediction[0][1] \n","    top_2 = prediction[1][1]\n","    output = [top_1 + \", \" + top_2]\n","    print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFupwWZEmqZj","executionInfo":{"status":"ok","timestamp":1686056678559,"user_tz":-420,"elapsed":8690,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"04eef5a5-9664-45c1-ac60-ca4c507ff988"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): jasa service hp\n","Text: jasa service hp\n","['200 - 250 Ribu, 20 - 50 Ribu']\n"]}]}]}