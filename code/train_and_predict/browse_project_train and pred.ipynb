{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vJlXjdqjkASpzAECOQGlGNiA811LOR8g","timestamp":1686057211652},{"file_id":"17_3PYD2cpTseU9bvW357RnhPST9keuV_","timestamp":1686056713488},{"file_id":"1ws14etu7cc0FHcr2y47jA7Pi3jzW-m_X","timestamp":1686055705128}],"authorship_tag":"ABX9TyNNVqzEUfSZlClYkOXvGV+h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"n4N6zSrBteyw"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"HtJdOKb6Ieja","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686057863669,"user_tz":-420,"elapsed":99675,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"42804f0c-8905-4057-935a-cc2766589fd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Epoch 1/50\n","2/2 [==============================] - 1s 24ms/step - loss: 0.7601 - binary_accuracy: 0.5289\n","Epoch 2/50\n","2/2 [==============================] - 0s 25ms/step - loss: 0.6213 - binary_accuracy: 0.6737\n","Epoch 3/50\n","2/2 [==============================] - 0s 20ms/step - loss: 0.5323 - binary_accuracy: 0.7430\n","Epoch 4/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.4308 - binary_accuracy: 0.8142\n","Epoch 5/50\n","2/2 [==============================] - 0s 26ms/step - loss: 0.3180 - binary_accuracy: 0.8755\n","Epoch 6/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.2150 - binary_accuracy: 0.9256\n","Epoch 7/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.1334 - binary_accuracy: 0.9655\n","Epoch 8/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0821 - binary_accuracy: 0.9828\n","Epoch 9/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0567 - binary_accuracy: 0.9898\n","Epoch 10/50\n","2/2 [==============================] - 0s 25ms/step - loss: 0.0469 - binary_accuracy: 0.9905\n","Epoch 11/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0431 - binary_accuracy: 0.9919\n","Epoch 12/50\n","2/2 [==============================] - 0s 23ms/step - loss: 0.0409 - binary_accuracy: 0.9918\n","Epoch 13/50\n","2/2 [==============================] - 0s 20ms/step - loss: 0.0364 - binary_accuracy: 0.9932\n","Epoch 14/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0321 - binary_accuracy: 0.9930\n","Epoch 15/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0285 - binary_accuracy: 0.9935\n","Epoch 16/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0250 - binary_accuracy: 0.9938\n","Epoch 17/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0218 - binary_accuracy: 0.9945\n","Epoch 18/50\n","2/2 [==============================] - 0s 24ms/step - loss: 0.0199 - binary_accuracy: 0.9950\n","Epoch 19/50\n","2/2 [==============================] - 0s 23ms/step - loss: 0.0186 - binary_accuracy: 0.9951\n","Epoch 20/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0172 - binary_accuracy: 0.9953\n","Epoch 21/50\n","2/2 [==============================] - 0s 23ms/step - loss: 0.0159 - binary_accuracy: 0.9956\n","Epoch 22/50\n","2/2 [==============================] - 0s 19ms/step - loss: 0.0148 - binary_accuracy: 0.9957\n","Epoch 23/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0137 - binary_accuracy: 0.9960\n","Epoch 24/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0127 - binary_accuracy: 0.9964\n","Epoch 25/50\n","2/2 [==============================] - 0s 20ms/step - loss: 0.0119 - binary_accuracy: 0.9964\n","Epoch 26/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0112 - binary_accuracy: 0.9967\n","Epoch 27/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0103 - binary_accuracy: 0.9968\n","Epoch 28/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0097 - binary_accuracy: 0.9970\n","Epoch 29/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0090 - binary_accuracy: 0.9972\n","Epoch 30/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0084 - binary_accuracy: 0.9974\n","Epoch 31/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0079 - binary_accuracy: 0.9977\n","Epoch 32/50\n","2/2 [==============================] - 0s 23ms/step - loss: 0.0074 - binary_accuracy: 0.9978\n","Epoch 33/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0070 - binary_accuracy: 0.9981\n","Epoch 34/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0065 - binary_accuracy: 0.9981\n","Epoch 35/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0061 - binary_accuracy: 0.9982\n","Epoch 36/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0057 - binary_accuracy: 0.9983\n","Epoch 37/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0054 - binary_accuracy: 0.9983\n","Epoch 38/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0051 - binary_accuracy: 0.9984\n","Epoch 39/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0048 - binary_accuracy: 0.9985\n","Epoch 40/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0045 - binary_accuracy: 0.9986\n","Epoch 41/50\n","2/2 [==============================] - 0s 20ms/step - loss: 0.0042 - binary_accuracy: 0.9987\n","Epoch 42/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0039 - binary_accuracy: 0.9989\n","Epoch 43/50\n","2/2 [==============================] - 0s 23ms/step - loss: 0.0037 - binary_accuracy: 0.9989\n","Epoch 44/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0035 - binary_accuracy: 0.9989\n","Epoch 45/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0033 - binary_accuracy: 0.9991\n","Epoch 46/50\n","2/2 [==============================] - 0s 24ms/step - loss: 0.0031 - binary_accuracy: 0.9991\n","Epoch 47/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0029 - binary_accuracy: 0.9993\n","Epoch 48/50\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0027 - binary_accuracy: 0.9993\n","Epoch 49/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0025 - binary_accuracy: 0.9993\n","Epoch 50/50\n","2/2 [==============================] - 0s 20ms/step - loss: 0.0024 - binary_accuracy: 0.9994\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]}],"source":["import logging\n","import re\n","import string\n","import time\n","import ast\n","import pandas as pd\n","from google.colab import drive\n","from typing import Tuple, Union, List, Dict\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","\n","level = logging.INFO\n","logging.basicConfig(level=level)\n","logger = logging.getLogger(__name__)\n","\n","\n","class TFModel(tf.Module):\n","    def __init__(self, model: tf.keras.Model) -> None:\n","        self.model = model\n","\n","class ModelTrainer:\n","    def __init__(self) -> None:\n","        self.tf_model_wrapper: TFModel\n","\n","        # Model Architecture parameters\n","        self.max_features = 50000\n","        self.epochs = 50\n","        self.batch_size = 64\n","        self.padding_token = \"<pad>\"\n","        self.auto = tf.data.AUTOTUNE\n","\n","    def read_train(self, dir_train):\n","      train_df = pd.read_csv(dir_train, index_col=0)\n","      train_df['projects_tags'] = train_df['projects_tags'].apply(ast.literal_eval)\n","      train_df['text'] = train_df['text'].apply(str)\n","      return train_df\n","\n","    def vocabulary_size(self, train_df):\n","      vocabulary = set()\n","      train_df[\"text\"].str.lower().str.split().apply(vocabulary.update)\n","      vocabulary_size = len(vocabulary)\n","      return vocabulary_size\n","\n","    def make_dataset(self, train_df, is_train=True):\n","      labels = tf.ragged.constant(train_df[\"projects_tags\"].values)\n","      lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n","      lookup.adapt(labels)\n","      label_binarized = lookup(labels).numpy()\n","      dataset = tf.data.Dataset.from_tensor_slices(\n","          (train_df[\"text\"].values, label_binarized)\n","        )\n","      dataset = dataset.shuffle(self.batch_size) if is_train else dataset\n","      return dataset.batch(self.batch_size)\n","\n","    def dataset(self, train_df):\n","      train_dataset = self.make_dataset(train_df, is_train = True)\n","      text_batch, label_batch = next(iter(train_dataset))\n","      text_batch = text_batch.numpy()\n","      label_batch = label_batch.numpy()\n","      return text_batch, label_batch\n","\n","    def init_vectorize_layer(self, vocabulary_size, text_dataset: np.ndarray) -> TextVectorization:\n","      text_vectorizer = TextVectorization(max_tokens=vocabulary_size,\n","                                          ngrams=2,\n","                                          output_mode='tf_idf')\n","      with tf.device(\"/CPU:0\"):\n","        text_vectorizer.adapt(text_dataset)\n","      return text_vectorizer\n","\n","    def init_model(self, train_df, vocabulary_size, text_dataset: np.ndarray) -> tf.keras.Model:\n","        text_batch, label_batch = self.dataset(train_df)\n","        vectorize_layer = self.init_vectorize_layer(text_dataset=text_batch, \n","                                                    vocabulary_size=vocabulary_size)\n","        raw_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n","        x = vectorize_layer(raw_input)\n","        x = tf.keras.layers.Dense(512, activation='relu')(x)\n","        x = tf.keras.layers.Dense(256, activation='relu')(x)\n","        x = tf.keras.layers.Dense(128, activation='relu')(x)\n","        predictions = tf.keras.layers.Dense(313, \n","                                            activation='sigmoid')(x)\n","        model = tf.keras.Model(raw_input, predictions)\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","        return model\n","\n","    def train(self) -> None:\n","        drive.mount('/content/drive')\n","        dir = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/projects_train_df.csv'  \n","        train_df = self.read_train(dir)\n","        vocabulary_size = self.vocabulary_size(train_df)\n","        text_batch, label_batch = self.dataset(train_df)\n","        model = self.init_model(train_df, text_dataset=text_batch, \n","                                vocabulary_size=vocabulary_size)\n","        model.fit(text_batch, label_batch, epochs=self.epochs)\n","        self.tf_model_wrapper = TFModel(model)\n","        path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/services/'\n","        model.save(path + 'worker/browse_projects_model/my_model')\n","\n","if __name__ == '__main__':\n","    model_trainer = ModelTrainer()\n","    model_trainer.train()"]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"JBtnMg0pkKYL"}},{"cell_type":"markdown","source":["### Load Model"],"metadata":{"id":"GDXPV7znklci"}},{"cell_type":"code","source":["path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/services/'\n","browse_project_model = tf.keras.models.load_model(path + 'worker/browse_projects_model/my_model')"],"metadata":{"id":"LbHtXUnckkxR","executionInfo":{"status":"ok","timestamp":1686057916829,"user_tz":-420,"elapsed":1374,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### read vocab and make predict"],"metadata":{"id":"bp1xqvPKktL3"}},{"cell_type":"code","source":["vocab = []\n","with open(r'/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/project_tags_vocab.txt', 'r') as fp:\n","  for line in fp:\n","    x = line[:-1]\n","    vocab.append(x)\n","\n","user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = browse_project_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = [x for _, x in sorted(zip(predicted_probabilities[i], vocab),\n","                                       key=lambda pair: pair[0],\n","                                       reverse=True)][:3]\n","    print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUO-nnwMkLnc","executionInfo":{"status":"ok","timestamp":1686057965259,"user_tz":-420,"elapsed":11215,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"a5d6d0ab-d087-48c7-b5df-0bb2acae5e2d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): jasa penbuatan landing page\n","['PHP', 'Web Programming', 'Website']\n"]}]},{"cell_type":"code","source":["user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = browse_project_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = sorted(zip(predicted_probabilities[i], vocab),\n","                        key=lambda pair: pair[0],\n","                        reverse=True)\n","    top_1 = prediction[0][1] \n","    top_2 = prediction[1][1]\n","    top_3 = prediction[2][1]\n","    output = [top_1 + \", \" + top_2 + ', ' + top_3]\n","    print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFupwWZEmqZj","executionInfo":{"status":"ok","timestamp":1686057975321,"user_tz":-420,"elapsed":5647,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"b86be83a-5a3d-4b5f-adce-b710acad3e28"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): jasa pembuatan landing page\n","['Web Programming, PHP, Website']\n"]}]}]}