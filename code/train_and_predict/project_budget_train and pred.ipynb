{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ws14etu7cc0FHcr2y47jA7Pi3jzW-m_X","timestamp":1685995042647}],"authorship_tag":"ABX9TyP5q/mNZyWl2pEwZV8LfI82"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"awJSrtgbkHqi"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"HtJdOKb6Ieja","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686046708035,"user_tz":-420,"elapsed":11057,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"f0db1b73-9609-4ca4-82fc-fa9da6463adb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 1/25\n","2/2 [==============================] - 1s 22ms/step - loss: 0.8135 - binary_accuracy: 0.6065\n","Epoch 2/25\n","2/2 [==============================] - 0s 23ms/step - loss: 0.3609 - binary_accuracy: 0.9100\n","Epoch 3/25\n","2/2 [==============================] - 0s 20ms/step - loss: 0.2207 - binary_accuracy: 0.9612\n","Epoch 4/25\n","2/2 [==============================] - 0s 23ms/step - loss: 0.1600 - binary_accuracy: 0.9725\n","Epoch 5/25\n","2/2 [==============================] - 0s 25ms/step - loss: 0.1286 - binary_accuracy: 0.9716\n","Epoch 6/25\n","2/2 [==============================] - 0s 22ms/step - loss: 0.1021 - binary_accuracy: 0.9744\n","Epoch 7/25\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0812 - binary_accuracy: 0.9825\n","Epoch 8/25\n","2/2 [==============================] - 0s 23ms/step - loss: 0.0671 - binary_accuracy: 0.9801\n","Epoch 9/25\n","2/2 [==============================] - 0s 20ms/step - loss: 0.0585 - binary_accuracy: 0.9830\n","Epoch 10/25\n","2/2 [==============================] - 0s 19ms/step - loss: 0.0481 - binary_accuracy: 0.9877\n","Epoch 11/25\n","2/2 [==============================] - 0s 19ms/step - loss: 0.0436 - binary_accuracy: 0.9901\n","Epoch 12/25\n","2/2 [==============================] - 0s 20ms/step - loss: 0.0381 - binary_accuracy: 0.9910\n","Epoch 13/25\n","2/2 [==============================] - 0s 24ms/step - loss: 0.0321 - binary_accuracy: 0.9920\n","Epoch 14/25\n","2/2 [==============================] - 0s 24ms/step - loss: 0.0274 - binary_accuracy: 0.9896\n","Epoch 15/25\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0241 - binary_accuracy: 0.9938\n","Epoch 16/25\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0207 - binary_accuracy: 0.9943\n","Epoch 17/25\n","2/2 [==============================] - 0s 29ms/step - loss: 0.0174 - binary_accuracy: 0.9957\n","Epoch 18/25\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0168 - binary_accuracy: 0.9962\n","Epoch 19/25\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0144 - binary_accuracy: 0.9967\n","Epoch 20/25\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0117 - binary_accuracy: 0.9976\n","Epoch 21/25\n","2/2 [==============================] - 0s 21ms/step - loss: 0.0110 - binary_accuracy: 0.9976\n","Epoch 22/25\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0098 - binary_accuracy: 0.9976\n","Epoch 23/25\n","2/2 [==============================] - 0s 22ms/step - loss: 0.0084 - binary_accuracy: 0.9976\n","Epoch 24/25\n","2/2 [==============================] - 0s 23ms/step - loss: 0.0074 - binary_accuracy: 0.9976\n","Epoch 25/25\n","2/2 [==============================] - 0s 30ms/step - loss: 0.0068 - binary_accuracy: 0.9976\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]}],"source":["import logging\n","import re\n","import string\n","import time\n","import ast\n","import pandas as pd\n","from google.colab import drive\n","from typing import Tuple, Union, List, Dict\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","\n","level = logging.INFO\n","logging.basicConfig(level=level)\n","logger = logging.getLogger(__name__)\n","\n","\n","class TFModel(tf.Module):\n","    def __init__(self, model: tf.keras.Model) -> None:\n","        self.model = model\n","\n","class ModelTrainer:\n","    def __init__(self) -> None:\n","        self.tf_model_wrapper: TFModel\n","\n","        # Model Architecture parameters\n","        self.max_features = 50000\n","        self.epochs = 25\n","        self.batch_size = 64\n","        self.padding_token = \"<pad>\"\n","        self.auto = tf.data.AUTOTUNE\n","\n","    def read_train(self, dir_train):\n","      train_df = pd.read_csv(dir_train, index_col=0)\n","      train_df['budget_tags'] = train_df['budget_tags'].apply(ast.literal_eval)\n","      train_df['text'] = train_df['text'].apply(str)\n","      return train_df\n","\n","    def vocabulary_size(self, train_df):\n","      vocabulary = set()\n","      train_df[\"text\"].str.lower().str.split().apply(vocabulary.update)\n","      vocabulary_size = len(vocabulary)\n","      return vocabulary_size\n","\n","    def make_dataset(self, train_df, is_train=True):\n","      labels = tf.ragged.constant(train_df[\"budget_tags\"].values)\n","      lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n","      lookup.adapt(labels)\n","      label_binarized = lookup(labels).numpy()\n","      dataset = tf.data.Dataset.from_tensor_slices(\n","          (train_df[\"text\"].values, label_binarized)\n","        )\n","      dataset = dataset.shuffle(self.batch_size) if is_train else dataset\n","      return dataset.batch(self.batch_size)\n","\n","    def dataset(self, train_df):\n","      train_dataset = self.make_dataset(train_df, is_train = True)\n","      text_batch, label_batch = next(iter(train_dataset))\n","      text_batch = text_batch.numpy()\n","      label_batch = label_batch.numpy()\n","      return text_batch, label_batch\n","\n","    def init_vectorize_layer(self, vocabulary_size, text_dataset: np.ndarray) -> TextVectorization:\n","      text_vectorizer = TextVectorization(max_tokens=vocabulary_size,\n","                                          ngrams=2,\n","                                          output_mode='tf_idf')\n","      with tf.device(\"/CPU:0\"):\n","        text_vectorizer.adapt(text_dataset)\n","      return text_vectorizer\n","\n","    def init_model(self, train_df, vocabulary_size, text_dataset: np.ndarray) -> tf.keras.Model:\n","        text_batch, label_batch = self.dataset(train_df)\n","        vectorize_layer = self.init_vectorize_layer(text_dataset=text_batch, \n","                                                    vocabulary_size=vocabulary_size)\n","        raw_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n","        x = vectorize_layer(raw_input)\n","        x = tf.keras.layers.Dense(512, activation='relu')(x)\n","        x = tf.keras.layers.Dense(256, activation='relu')(x)\n","        x = tf.keras.layers.Dense(128, activation='relu')(x)\n","        predictions = tf.keras.layers.Dense(33, \n","                                            activation='sigmoid')(x)\n","        model = tf.keras.Model(raw_input, predictions)\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","        return model\n","\n","    def train(self) -> None:\n","        drive.mount('/content/drive')\n","        dir = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/projects_budget_train_df.csv'  \n","        train_df = self.read_train(dir)\n","        vocabulary_size = self.vocabulary_size(train_df)\n","        text_batch, label_batch = self.dataset(train_df)\n","        model = self.init_model(train_df, text_dataset=text_batch, \n","                                vocabulary_size=vocabulary_size)\n","        model.fit(text_batch, label_batch, epochs=self.epochs)\n","        self.tf_model_wrapper = TFModel(model)\n","        path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/projects/'\n","        model.save(path + 'budget_label_model/my_model')\n","        logger.info('saving SavedModel to project_budget/my_models')\n","\n","if __name__ == '__main__':\n","    model_trainer = ModelTrainer()\n","    model_trainer.train()"]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"JBtnMg0pkKYL"}},{"cell_type":"markdown","source":["### Load Model"],"metadata":{"id":"GDXPV7znklci"}},{"cell_type":"code","source":["path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/projects/'\n","budget_label_model = tf.keras.models.load_model(path + 'budget_label_model/my_model')"],"metadata":{"id":"LbHtXUnckkxR","executionInfo":{"status":"ok","timestamp":1686052859160,"user_tz":-420,"elapsed":589,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### read vocab and make predict"],"metadata":{"id":"bp1xqvPKktL3"}},{"cell_type":"code","source":["vocab = []\n","with open(r'/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/projects_budget_vocab.txt', 'r') as fp:\n","  for line in fp:\n","    x = line[:-1]\n","    vocab.append(x)\n","\n","user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = budget_label_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = [x for _, x in sorted(zip(predicted_probabilities[i], vocab),\n","                                       key=lambda pair: pair[0],\n","                                       reverse=True)][:2]\n","    print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUO-nnwMkLnc","executionInfo":{"status":"ok","timestamp":1686053251087,"user_tz":-420,"elapsed":10401,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"ff029df1-c74d-4680-f9ea-a651f104350f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): desain \n","['250 - 300 Ribu', '50 - 100 Ribu']\n"]}]},{"cell_type":"code","source":["user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = budget_label_model(user_input)\n","for i, text in enumerate(user_input):\n","    print(f\"Text: {text}\")\n","    prediction = sorted(zip(predicted_probabilities[i], vocab),\n","                        key=lambda pair: pair[0],\n","                        reverse=True)\n","    top_1 = prediction[0][1] \n","    top_2 = prediction[1][1]\n","    output = [top_1 + \", \" + top_2]\n","    print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFupwWZEmqZj","executionInfo":{"status":"ok","timestamp":1686053512757,"user_tz":-420,"elapsed":6479,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"ea50be83-2260-47fe-be80-b6872ff51fc8"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): service laptop\n","Text: service laptop\n","['100 - 150 Ribu, 900 Ribu -  1 Juta']\n"]}]}]}