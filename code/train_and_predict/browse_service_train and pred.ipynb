{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17_3PYD2cpTseU9bvW357RnhPST9keuV_","timestamp":1686056713488},{"file_id":"1ws14etu7cc0FHcr2y47jA7Pi3jzW-m_X","timestamp":1686055705128}],"authorship_tag":"ABX9TyPB1C/1Sl/u5ejAWebDVJui"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"n4N6zSrBteyw"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"HtJdOKb6Ieja","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686057103142,"user_tz":-420,"elapsed":21066,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"9e69a48d-8cdc-4595-9224-f208b5f8a5bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 1/50\n","2/2 [==============================] - 2s 78ms/step - loss: 0.7796 - binary_accuracy: 0.5468\n","Epoch 2/50\n","2/2 [==============================] - 0s 67ms/step - loss: 0.5968 - binary_accuracy: 0.7167\n","Epoch 3/50\n","2/2 [==============================] - 0s 69ms/step - loss: 0.4998 - binary_accuracy: 0.7842\n","Epoch 4/50\n","2/2 [==============================] - 0s 76ms/step - loss: 0.3858 - binary_accuracy: 0.8477\n","Epoch 5/50\n","2/2 [==============================] - 0s 66ms/step - loss: 0.2715 - binary_accuracy: 0.9128\n","Epoch 6/50\n","2/2 [==============================] - 0s 62ms/step - loss: 0.1780 - binary_accuracy: 0.9491\n","Epoch 7/50\n","2/2 [==============================] - 0s 66ms/step - loss: 0.1151 - binary_accuracy: 0.9747\n","Epoch 8/50\n","2/2 [==============================] - 0s 81ms/step - loss: 0.0830 - binary_accuracy: 0.9874\n","Epoch 9/50\n","2/2 [==============================] - 0s 59ms/step - loss: 0.0714 - binary_accuracy: 0.9903\n","Epoch 10/50\n","2/2 [==============================] - 0s 51ms/step - loss: 0.0655 - binary_accuracy: 0.9905\n","Epoch 11/50\n","2/2 [==============================] - 0s 63ms/step - loss: 0.0593 - binary_accuracy: 0.9904\n","Epoch 12/50\n","2/2 [==============================] - 0s 71ms/step - loss: 0.0516 - binary_accuracy: 0.9907\n","Epoch 13/50\n","2/2 [==============================] - 0s 58ms/step - loss: 0.0447 - binary_accuracy: 0.9910\n","Epoch 14/50\n","2/2 [==============================] - 0s 52ms/step - loss: 0.0392 - binary_accuracy: 0.9918\n","Epoch 15/50\n","2/2 [==============================] - 0s 74ms/step - loss: 0.0353 - binary_accuracy: 0.9921\n","Epoch 16/50\n","2/2 [==============================] - 0s 65ms/step - loss: 0.0320 - binary_accuracy: 0.9923\n","Epoch 17/50\n","2/2 [==============================] - 0s 68ms/step - loss: 0.0288 - binary_accuracy: 0.9926\n","Epoch 18/50\n","2/2 [==============================] - 0s 59ms/step - loss: 0.0259 - binary_accuracy: 0.9934\n","Epoch 19/50\n","2/2 [==============================] - 0s 50ms/step - loss: 0.0236 - binary_accuracy: 0.9939\n","Epoch 20/50\n","2/2 [==============================] - 0s 62ms/step - loss: 0.0211 - binary_accuracy: 0.9942\n","Epoch 21/50\n","2/2 [==============================] - 0s 49ms/step - loss: 0.0193 - binary_accuracy: 0.9947\n","Epoch 22/50\n","2/2 [==============================] - 0s 64ms/step - loss: 0.0176 - binary_accuracy: 0.9947\n","Epoch 23/50\n","2/2 [==============================] - 0s 47ms/step - loss: 0.0158 - binary_accuracy: 0.9950\n","Epoch 24/50\n","2/2 [==============================] - 0s 47ms/step - loss: 0.0143 - binary_accuracy: 0.9956\n","Epoch 25/50\n","2/2 [==============================] - 0s 48ms/step - loss: 0.0128 - binary_accuracy: 0.9959\n","Epoch 26/50\n","2/2 [==============================] - 0s 46ms/step - loss: 0.0115 - binary_accuracy: 0.9963\n","Epoch 27/50\n","2/2 [==============================] - 0s 48ms/step - loss: 0.0104 - binary_accuracy: 0.9967\n","Epoch 28/50\n","2/2 [==============================] - 0s 49ms/step - loss: 0.0094 - binary_accuracy: 0.9968\n","Epoch 29/50\n","2/2 [==============================] - 0s 51ms/step - loss: 0.0084 - binary_accuracy: 0.9974\n","Epoch 30/50\n","2/2 [==============================] - 0s 60ms/step - loss: 0.0077 - binary_accuracy: 0.9978\n","Epoch 31/50\n","2/2 [==============================] - 0s 60ms/step - loss: 0.0070 - binary_accuracy: 0.9980\n","Epoch 32/50\n","2/2 [==============================] - 0s 60ms/step - loss: 0.0065 - binary_accuracy: 0.9983\n","Epoch 33/50\n","2/2 [==============================] - 0s 66ms/step - loss: 0.0059 - binary_accuracy: 0.9984\n","Epoch 34/50\n","2/2 [==============================] - 0s 47ms/step - loss: 0.0054 - binary_accuracy: 0.9988\n","Epoch 35/50\n","2/2 [==============================] - 0s 48ms/step - loss: 0.0049 - binary_accuracy: 0.9991\n","Epoch 36/50\n","2/2 [==============================] - 0s 57ms/step - loss: 0.0045 - binary_accuracy: 0.9990\n","Epoch 37/50\n","2/2 [==============================] - 0s 64ms/step - loss: 0.0041 - binary_accuracy: 0.9989\n","Epoch 38/50\n","2/2 [==============================] - 0s 49ms/step - loss: 0.0038 - binary_accuracy: 0.9992\n","Epoch 39/50\n","2/2 [==============================] - 0s 45ms/step - loss: 0.0034 - binary_accuracy: 0.9994\n","Epoch 40/50\n","2/2 [==============================] - 0s 58ms/step - loss: 0.0032 - binary_accuracy: 0.9993\n","Epoch 41/50\n","2/2 [==============================] - 0s 44ms/step - loss: 0.0029 - binary_accuracy: 0.9993\n","Epoch 42/50\n","2/2 [==============================] - 0s 39ms/step - loss: 0.0026 - binary_accuracy: 0.9994\n","Epoch 43/50\n","2/2 [==============================] - 0s 37ms/step - loss: 0.0024 - binary_accuracy: 0.9994\n","Epoch 44/50\n","2/2 [==============================] - 0s 46ms/step - loss: 0.0023 - binary_accuracy: 0.9995\n","Epoch 45/50\n","2/2 [==============================] - 0s 38ms/step - loss: 0.0021 - binary_accuracy: 0.9996\n","Epoch 46/50\n","2/2 [==============================] - 0s 44ms/step - loss: 0.0019 - binary_accuracy: 0.9998\n","Epoch 47/50\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0018 - binary_accuracy: 0.9999\n","Epoch 48/50\n","2/2 [==============================] - 0s 41ms/step - loss: 0.0016 - binary_accuracy: 0.9999\n","Epoch 49/50\n","2/2 [==============================] - 0s 34ms/step - loss: 0.0015 - binary_accuracy: 0.9999\n","Epoch 50/50\n","2/2 [==============================] - 0s 34ms/step - loss: 0.0014 - binary_accuracy: 0.9999\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]}],"source":["import logging\n","import re\n","import string\n","import time\n","import ast\n","import pandas as pd\n","from google.colab import drive\n","from typing import Tuple, Union, List, Dict\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","\n","level = logging.INFO\n","logging.basicConfig(level=level)\n","logger = logging.getLogger(__name__)\n","\n","\n","class TFModel(tf.Module):\n","    def __init__(self, model: tf.keras.Model) -> None:\n","        self.model = model\n","\n","class ModelTrainer:\n","    def __init__(self) -> None:\n","        self.tf_model_wrapper: TFModel\n","\n","        # Model Architecture parameters\n","        self.max_features = 50000\n","        self.epochs = 50\n","        self.batch_size = 64\n","        self.padding_token = \"<pad>\"\n","        self.auto = tf.data.AUTOTUNE\n","\n","    def read_train(self, dir_train):\n","      train_df = pd.read_csv(dir_train, index_col=0)\n","      train_df['service_tags'] = train_df['service_tags'].apply(ast.literal_eval)\n","      train_df['text'] = train_df['text'].apply(str)\n","      return train_df\n","\n","    def vocabulary_size(self, train_df):\n","      vocabulary = set()\n","      train_df[\"text\"].str.lower().str.split().apply(vocabulary.update)\n","      vocabulary_size = len(vocabulary)\n","      return vocabulary_size\n","\n","    def make_dataset(self, train_df, is_train=True):\n","      labels = tf.ragged.constant(train_df[\"service_tags\"].values)\n","      lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n","      lookup.adapt(labels)\n","      label_binarized = lookup(labels).numpy()\n","      dataset = tf.data.Dataset.from_tensor_slices(\n","          (train_df[\"text\"].values, label_binarized)\n","        )\n","      dataset = dataset.shuffle(self.batch_size) if is_train else dataset\n","      return dataset.batch(self.batch_size)\n","\n","    def dataset(self, train_df):\n","      train_dataset = self.make_dataset(train_df, is_train = True)\n","      text_batch, label_batch = next(iter(train_dataset))\n","      text_batch = text_batch.numpy()\n","      label_batch = label_batch.numpy()\n","      return text_batch, label_batch\n","\n","    def init_vectorize_layer(self, vocabulary_size, text_dataset: np.ndarray) -> TextVectorization:\n","      text_vectorizer = TextVectorization(max_tokens=vocabulary_size,\n","                                          ngrams=2,\n","                                          output_mode='tf_idf')\n","      with tf.device(\"/CPU:0\"):\n","        text_vectorizer.adapt(text_dataset)\n","      return text_vectorizer\n","\n","    def init_model(self, train_df, vocabulary_size, text_dataset: np.ndarray) -> tf.keras.Model:\n","        text_batch, label_batch = self.dataset(train_df)\n","        vectorize_layer = self.init_vectorize_layer(text_dataset=text_batch, \n","                                                    vocabulary_size=vocabulary_size)\n","        raw_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n","        x = vectorize_layer(raw_input)\n","        x = tf.keras.layers.Dense(512, activation='relu')(x)\n","        x = tf.keras.layers.Dense(256, activation='relu')(x)\n","        x = tf.keras.layers.Dense(128, activation='relu')(x)\n","        predictions = tf.keras.layers.Dense(270, \n","                                            activation='sigmoid')(x)\n","        model = tf.keras.Model(raw_input, predictions)\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","        return model\n","\n","    def train(self) -> None:\n","        drive.mount('/content/drive')\n","        dir = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/services_train_df.csv'  \n","        train_df = self.read_train(dir)\n","        vocabulary_size = self.vocabulary_size(train_df)\n","        text_batch, label_batch = self.dataset(train_df)\n","        model = self.init_model(train_df, text_dataset=text_batch, \n","                                vocabulary_size=vocabulary_size)\n","        model.fit(text_batch, label_batch, epochs=self.epochs)\n","        self.tf_model_wrapper = TFModel(model)\n","        path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/projects/'\n","        model.save(path + 'owner/browse_service_model/my_model')\n","\n","if __name__ == '__main__':\n","    model_trainer = ModelTrainer()\n","    model_trainer.train()"]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"JBtnMg0pkKYL"}},{"cell_type":"markdown","source":["### Load Model"],"metadata":{"id":"GDXPV7znklci"}},{"cell_type":"code","source":["path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/projects/'\n","browse_service_model = tf.keras.models.load_model(path + 'owner/browse_service_model/my_model')"],"metadata":{"id":"LbHtXUnckkxR","executionInfo":{"status":"ok","timestamp":1686057135598,"user_tz":-420,"elapsed":1331,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### read vocab and make predict"],"metadata":{"id":"bp1xqvPKktL3"}},{"cell_type":"code","source":["vocab = []\n","with open(r'/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/service_tags_vocab.txt', 'r') as fp:\n","  for line in fp:\n","    x = line[:-1]\n","    vocab.append(x)\n","\n","user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = browse_service_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = [x for _, x in sorted(zip(predicted_probabilities[i], vocab),\n","                                       key=lambda pair: pair[0],\n","                                       reverse=True)][:3]\n","    print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUO-nnwMkLnc","executionInfo":{"status":"ok","timestamp":1686057170834,"user_tz":-420,"elapsed":7578,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"d04e5018-bd67-480e-e854-b86b0de6b194"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): saya punya proyek pembuatan website\n","['Website Building', 'Wordpress', 'Wordpress Templates']\n"]}]},{"cell_type":"code","source":["user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = browse_service_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = sorted(zip(predicted_probabilities[i], vocab),\n","                        key=lambda pair: pair[0],\n","                        reverse=True)\n","    top_1 = prediction[0][1] \n","    top_2 = prediction[1][1]\n","    top_3 = prediction[2][1]\n","    output = [top_1 + \", \" + top_2 + ', ' + top_3]\n","    print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFupwWZEmqZj","executionInfo":{"status":"ok","timestamp":1686057192184,"user_tz":-420,"elapsed":8460,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"c55f7545-2851-4f13-e584-877ed4506183"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): saya punya proyek pembuatan website\n","['Website Building, Wordpress, Wordpress Templates']\n"]}]}]}