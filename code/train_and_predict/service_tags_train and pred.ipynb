{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ws14etu7cc0FHcr2y47jA7Pi3jzW-m_X","timestamp":1686055705128}],"authorship_tag":"ABX9TyPxEXbcebqIkFUmlw1TqK4w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"n4N6zSrBteyw"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"HtJdOKb6Ieja","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686056178143,"user_tz":-420,"elapsed":20196,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"6c2cae42-6dbe-4754-bf34-6a08401d0cf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 1/50\n","2/2 [==============================] - 2s 95ms/step - loss: 0.8172 - binary_accuracy: 0.5347\n","Epoch 2/50\n","2/2 [==============================] - 0s 97ms/step - loss: 0.6180 - binary_accuracy: 0.6652\n","Epoch 3/50\n","2/2 [==============================] - 0s 69ms/step - loss: 0.5206 - binary_accuracy: 0.7496\n","Epoch 4/50\n","2/2 [==============================] - 0s 80ms/step - loss: 0.4195 - binary_accuracy: 0.8194\n","Epoch 5/50\n","2/2 [==============================] - 0s 81ms/step - loss: 0.3129 - binary_accuracy: 0.8850\n","Epoch 6/50\n","2/2 [==============================] - 0s 87ms/step - loss: 0.2169 - binary_accuracy: 0.9310\n","Epoch 7/50\n","2/2 [==============================] - 0s 69ms/step - loss: 0.1392 - binary_accuracy: 0.9627\n","Epoch 8/50\n","2/2 [==============================] - 0s 100ms/step - loss: 0.0894 - binary_accuracy: 0.9834\n","Epoch 9/50\n","2/2 [==============================] - 0s 70ms/step - loss: 0.0682 - binary_accuracy: 0.9907\n","Epoch 10/50\n","2/2 [==============================] - 0s 75ms/step - loss: 0.0606 - binary_accuracy: 0.9909\n","Epoch 11/50\n","2/2 [==============================] - 0s 63ms/step - loss: 0.0555 - binary_accuracy: 0.9911\n","Epoch 12/50\n","2/2 [==============================] - 0s 97ms/step - loss: 0.0501 - binary_accuracy: 0.9911\n","Epoch 13/50\n","2/2 [==============================] - 0s 69ms/step - loss: 0.0451 - binary_accuracy: 0.9917\n","Epoch 14/50\n","2/2 [==============================] - 0s 76ms/step - loss: 0.0412 - binary_accuracy: 0.9920\n","Epoch 15/50\n","2/2 [==============================] - 0s 77ms/step - loss: 0.0366 - binary_accuracy: 0.9919\n","Epoch 16/50\n","2/2 [==============================] - 0s 59ms/step - loss: 0.0325 - binary_accuracy: 0.9921\n","Epoch 17/50\n","2/2 [==============================] - 0s 44ms/step - loss: 0.0298 - binary_accuracy: 0.9931\n","Epoch 18/50\n","2/2 [==============================] - 0s 45ms/step - loss: 0.0282 - binary_accuracy: 0.9932\n","Epoch 19/50\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0254 - binary_accuracy: 0.9934\n","Epoch 20/50\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0228 - binary_accuracy: 0.9938\n","Epoch 21/50\n","2/2 [==============================] - 0s 39ms/step - loss: 0.0206 - binary_accuracy: 0.9942\n","Epoch 22/50\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0187 - binary_accuracy: 0.9946\n","Epoch 23/50\n","2/2 [==============================] - 0s 42ms/step - loss: 0.0171 - binary_accuracy: 0.9952\n","Epoch 24/50\n","2/2 [==============================] - 0s 43ms/step - loss: 0.0158 - binary_accuracy: 0.9957\n","Epoch 25/50\n","2/2 [==============================] - 0s 39ms/step - loss: 0.0142 - binary_accuracy: 0.9962\n","Epoch 26/50\n","2/2 [==============================] - 0s 39ms/step - loss: 0.0131 - binary_accuracy: 0.9965\n","Epoch 27/50\n","2/2 [==============================] - 0s 41ms/step - loss: 0.0118 - binary_accuracy: 0.9966\n","Epoch 28/50\n","2/2 [==============================] - 0s 38ms/step - loss: 0.0105 - binary_accuracy: 0.9970\n","Epoch 29/50\n","2/2 [==============================] - 0s 39ms/step - loss: 0.0096 - binary_accuracy: 0.9973\n","Epoch 30/50\n","2/2 [==============================] - 0s 57ms/step - loss: 0.0088 - binary_accuracy: 0.9976\n","Epoch 31/50\n","2/2 [==============================] - 0s 55ms/step - loss: 0.0080 - binary_accuracy: 0.9980\n","Epoch 32/50\n","2/2 [==============================] - 0s 63ms/step - loss: 0.0072 - binary_accuracy: 0.9981\n","Epoch 33/50\n","2/2 [==============================] - 0s 69ms/step - loss: 0.0066 - binary_accuracy: 0.9986\n","Epoch 34/50\n","2/2 [==============================] - 0s 64ms/step - loss: 0.0060 - binary_accuracy: 0.9987\n","Epoch 35/50\n","2/2 [==============================] - 0s 73ms/step - loss: 0.0054 - binary_accuracy: 0.9989\n","Epoch 36/50\n","2/2 [==============================] - 0s 62ms/step - loss: 0.0049 - binary_accuracy: 0.9991\n","Epoch 37/50\n","2/2 [==============================] - 0s 50ms/step - loss: 0.0045 - binary_accuracy: 0.9992\n","Epoch 38/50\n","2/2 [==============================] - 0s 64ms/step - loss: 0.0041 - binary_accuracy: 0.9993\n","Epoch 39/50\n","2/2 [==============================] - 0s 51ms/step - loss: 0.0037 - binary_accuracy: 0.9994\n","Epoch 40/50\n","2/2 [==============================] - 0s 62ms/step - loss: 0.0034 - binary_accuracy: 0.9995\n","Epoch 41/50\n","2/2 [==============================] - 0s 60ms/step - loss: 0.0031 - binary_accuracy: 0.9995\n","Epoch 42/50\n","2/2 [==============================] - 0s 57ms/step - loss: 0.0029 - binary_accuracy: 0.9997\n","Epoch 43/50\n","2/2 [==============================] - 0s 58ms/step - loss: 0.0027 - binary_accuracy: 0.9997\n","Epoch 44/50\n","2/2 [==============================] - 0s 52ms/step - loss: 0.0025 - binary_accuracy: 0.9998\n","Epoch 45/50\n","2/2 [==============================] - 0s 63ms/step - loss: 0.0023 - binary_accuracy: 0.9998\n","Epoch 46/50\n","2/2 [==============================] - 0s 60ms/step - loss: 0.0021 - binary_accuracy: 0.9998\n","Epoch 47/50\n","2/2 [==============================] - 0s 57ms/step - loss: 0.0020 - binary_accuracy: 0.9998\n","Epoch 48/50\n","2/2 [==============================] - 0s 57ms/step - loss: 0.0018 - binary_accuracy: 0.9998\n","Epoch 49/50\n","2/2 [==============================] - 0s 48ms/step - loss: 0.0017 - binary_accuracy: 0.9998\n","Epoch 50/50\n","2/2 [==============================] - 0s 77ms/step - loss: 0.0016 - binary_accuracy: 0.9998\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]}],"source":["import logging\n","import re\n","import string\n","import time\n","import ast\n","import pandas as pd\n","from google.colab import drive\n","from typing import Tuple, Union, List, Dict\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","\n","level = logging.INFO\n","logging.basicConfig(level=level)\n","logger = logging.getLogger(__name__)\n","\n","\n","class TFModel(tf.Module):\n","    def __init__(self, model: tf.keras.Model) -> None:\n","        self.model = model\n","\n","class ModelTrainer:\n","    def __init__(self) -> None:\n","        self.tf_model_wrapper: TFModel\n","\n","        # Model Architecture parameters\n","        self.max_features = 50000\n","        self.epochs = 50\n","        self.batch_size = 64\n","        self.padding_token = \"<pad>\"\n","        self.auto = tf.data.AUTOTUNE\n","\n","    def read_train(self, dir_train):\n","      train_df = pd.read_csv(dir_train, index_col=0)\n","      train_df['service_tags'] = train_df['service_tags'].apply(ast.literal_eval)\n","      train_df['text'] = train_df['text'].apply(str)\n","      return train_df\n","\n","    def vocabulary_size(self, train_df):\n","      vocabulary = set()\n","      train_df[\"text\"].str.lower().str.split().apply(vocabulary.update)\n","      vocabulary_size = len(vocabulary)\n","      return vocabulary_size\n","\n","    def make_dataset(self, train_df, is_train=True):\n","      labels = tf.ragged.constant(train_df[\"service_tags\"].values)\n","      lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n","      lookup.adapt(labels)\n","      label_binarized = lookup(labels).numpy()\n","      dataset = tf.data.Dataset.from_tensor_slices(\n","          (train_df[\"text\"].values, label_binarized)\n","        )\n","      dataset = dataset.shuffle(self.batch_size) if is_train else dataset\n","      return dataset.batch(self.batch_size)\n","\n","    def dataset(self, train_df):\n","      train_dataset = self.make_dataset(train_df, is_train = True)\n","      text_batch, label_batch = next(iter(train_dataset))\n","      text_batch = text_batch.numpy()\n","      label_batch = label_batch.numpy()\n","      return text_batch, label_batch\n","\n","    def init_vectorize_layer(self, vocabulary_size, text_dataset: np.ndarray) -> TextVectorization:\n","      text_vectorizer = TextVectorization(max_tokens=vocabulary_size,\n","                                          ngrams=2,\n","                                          output_mode='tf_idf')\n","      with tf.device(\"/CPU:0\"):\n","        text_vectorizer.adapt(text_dataset)\n","      return text_vectorizer\n","\n","    def init_model(self, train_df, vocabulary_size, text_dataset: np.ndarray) -> tf.keras.Model:\n","        text_batch, label_batch = self.dataset(train_df)\n","        vectorize_layer = self.init_vectorize_layer(text_dataset=text_batch, \n","                                                    vocabulary_size=vocabulary_size)\n","        raw_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n","        x = vectorize_layer(raw_input)\n","        x = tf.keras.layers.Dense(512, activation='relu')(x)\n","        x = tf.keras.layers.Dense(256, activation='relu')(x)\n","        x = tf.keras.layers.Dense(128, activation='relu')(x)\n","        predictions = tf.keras.layers.Dense(270, \n","                                            activation='sigmoid')(x)\n","        model = tf.keras.Model(raw_input, predictions)\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","        return model\n","\n","    def train(self) -> None:\n","        drive.mount('/content/drive')\n","        dir = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/service_tags_train_df.csv'  \n","        train_df = self.read_train(dir)\n","        vocabulary_size = self.vocabulary_size(train_df)\n","        text_batch, label_batch = self.dataset(train_df)\n","        model = self.init_model(train_df, text_dataset=text_batch, \n","                                vocabulary_size=vocabulary_size)\n","        model.fit(text_batch, label_batch, epochs=self.epochs)\n","        self.tf_model_wrapper = TFModel(model)\n","        path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/services/'\n","        model.save(path + 'service_tags_model/my_model')\n","\n","if __name__ == '__main__':\n","    model_trainer = ModelTrainer()\n","    model_trainer.train()"]},{"cell_type":"markdown","source":["# Predict"],"metadata":{"id":"JBtnMg0pkKYL"}},{"cell_type":"markdown","source":["### Load Model"],"metadata":{"id":"GDXPV7znklci"}},{"cell_type":"code","source":["path = '/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/model/services/'\n","service_tags_model = tf.keras.models.load_model(path + 'service_tags_model/my_model')"],"metadata":{"id":"LbHtXUnckkxR","executionInfo":{"status":"ok","timestamp":1686056223945,"user_tz":-420,"elapsed":1030,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### read vocab and make predict"],"metadata":{"id":"bp1xqvPKktL3"}},{"cell_type":"code","source":["vocab = []\n","with open(r'/content/drive/Shareddrives/Capstone Project/Product-based/ml-stuff/data/service_tags_vocab.txt', 'r') as fp:\n","  for line in fp:\n","    x = line[:-1]\n","    vocab.append(x)\n","\n","user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = service_tags_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = [x for _, x in sorted(zip(predicted_probabilities[i], vocab),\n","                                       key=lambda pair: pair[0],\n","                                       reverse=True)][:3]\n","    print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUO-nnwMkLnc","executionInfo":{"status":"ok","timestamp":1686056255685,"user_tz":-420,"elapsed":31786,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"d63b9d03-ce4d-4a42-84e4-c54d4fae2545"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): entry data\n","['Data Entry', 'Logo Design', 'Video Editing']\n"]}]},{"cell_type":"code","source":["user_input = pd.Series(str(input('Text (S): ')))\n","predicted_probabilities = service_tags_model(user_input)\n","for i, text in enumerate(user_input):\n","    prediction = sorted(zip(predicted_probabilities[i], vocab),\n","                        key=lambda pair: pair[0],\n","                        reverse=True)\n","    top_1 = prediction[0][1] \n","    top_2 = prediction[1][1]\n","    top_3 = prediction[2][1]\n","    output = [top_1 + \", \" + top_2 + ', ' + top_3]\n","    print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFupwWZEmqZj","executionInfo":{"status":"ok","timestamp":1686056274038,"user_tz":-420,"elapsed":4486,"user":{"displayName":"Prasetyo Adi Nugroho M169DKX3777","userId":"08061643827470737563"}},"outputId":"02fa17ec-0c28-4ae9-8921-6ead7e1854d9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Text (S): entry data\n","['Data Entry, Logo Design, Video Editing']\n"]}]}]}